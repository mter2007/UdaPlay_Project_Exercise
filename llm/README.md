# Local AI Agent with Ollama

A simple Python AI agent project that runs language models locally using Ollama.

## ğŸ“‹ Prerequisites

- Python 3.8+
- Ollama (https://ollama.ai)

## ğŸš€ Quick Start

### 1. Install Ollama
Download and install from [ollama.ai](https://ollama.ai)

### 2. Start Ollama Server
```bash
ollama serve
```

### 3. Pull a Model (in another terminal)
```bash
ollama pull mistral
```

Recommended models:
- `mistral` - Fast and efficient (7B)
- `llama2` - High quality (7B)
- `neural-chat` - Good for conversations (7B)
- `tinyllama` - Lightweight (1.1B)

### 4. Set Up Project

```bash
# Create .env file from example
cp .env.example .env

# Install dependencies
pip install -r requirements.txt
```

### 5. Run the Agent

```bash
# Interactive chat mode
python main.py

# Or use the agent in your code
python agent.py
```

## ğŸ“ Project Structure

```
llm/
â”œâ”€â”€ agent.py           # Core OllamaAgent class
â”œâ”€â”€ main.py            # Interactive chat demo
â”œâ”€â”€ requirements.txt   # Python dependencies
â”œâ”€â”€ .env.example       # Environment variables template
â””â”€â”€ README.md          # This file
```

## ğŸ’» Usage

### Simple Generation
```python
from agent import OllamaAgent

agent = OllamaAgent()
response = agent.generate("What is AI?")
print(response)
```

### Chat Conversation
```python
messages = [
    {"role": "user", "content": "Hello!"},
]
response = agent.chat(messages)
messages.append({"role": "assistant", "content": response})

# Continue the conversation
messages.append({"role": "user", "content": "How are you?"})
response = agent.chat(messages)
```

## ğŸ”§ Configuration

Edit `.env` file to customize:
```
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=mistral
```

## ğŸ“š Resources

- Ollama Documentation: https://github.com/ollama/ollama
- Available Models: https://ollama.ai/library
- API Reference: https://github.com/ollama/ollama/blob/main/docs/api.md

## âœ… Troubleshooting

- **"Connection refused"**: Make sure `ollama serve` is running
- **"Model not found"**: Run `ollama pull <model-name>`
- **Slow responses**: Use a smaller model like `tinyllama`
- **High memory usage**: Switch to a lighter model

Happy coding! ğŸ‰
